{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "## Udacity Capstone Project\n",
    "\n",
    "#### ProjectSummary.md has the writeup for this project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import configparser\n",
    "import datetime as dt\n",
    "\n",
    "from pyspark.sql import SparkSession, SQLContext, GroupedData, HiveContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.functions import date_add as d_add\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import year, month, dayofmonth, weekofyear, date_format\n",
    "from pyspark.sql import functions as func\n",
    "from pyspark.sql.functions import monotonically_increasing_id\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Function definitions to create data models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def load_immigration_data(df, spark):\n",
    "    # df = spark.read.parquet(input_path)\n",
    "    \n",
    "    df = df \\\n",
    "    .withColumn(\"cicid\", col(\"cicid\").cast(\"integer\")) \\\n",
    "    .withColumn(\"year\", col(\"i94yr\").cast(\"integer\")) \\\n",
    "    .drop(\"i94yr\") \\\n",
    "    .withColumn(\"month\", col(\"i94mon\").cast(\"integer\")) \\\n",
    "    .drop(\"i94mon\") \\\n",
    "    .withColumn(\"bornCountry\", col(\"i94cit\").cast(\"integer\")) \\\n",
    "    .drop(\"i94cit\") \\\n",
    "    .withColumn(\"residentCountry\", col(\"i94res\").cast(\"integer\")) \\\n",
    "    .drop(\"i94res\") \\\n",
    "    .withColumnRenamed(\"i94port\", \"arrivalPort\") \\\n",
    "    .withColumn(\"mode\", col(\"i94mode\").cast(\"integer\")) \\\n",
    "    .drop(\"i94mode\") \\\n",
    "    .withColumnRenamed(\"i94addr\", \"state_code\") \\\n",
    "    .withColumn(\"age\", col(\"i94bir\").cast(\"integer\")) \\\n",
    "    .drop(\"i94bir\") \\\n",
    "    .withColumn(\"visa\", col(\"i94visa\").cast(\"integer\")) \\\n",
    "    .drop(\"i94visa\") \\\n",
    "    .withColumnRenamed(\"entdepa\", \"arrivalFlag\") \\\n",
    "    .withColumnRenamed(\"entdepd\", \"departureFlag\") \\\n",
    "    .withColumnRenamed(\"entdepu\", \"updateFlag\") \\\n",
    "    .withColumnRenamed(\"matflag\", \"matchFlag\") \\\n",
    "    .withColumn(\"birthYear\", col(\"biryear\").cast(\"integer\")) \\\n",
    "    .drop(\"biryear\") \\\n",
    "    .withColumnRenamed(\"fltno\", \"flightNumber\") \\\n",
    "    .withColumn(\"sasDate\", to_date(lit(\"01/01/1960\"), \"MM/dd/yyyy\")) \\\n",
    "    .withColumn(\"arrivalDate\", expr(\"date_add(sasDate, arrdate)\")) \\\n",
    "    .withColumn(\"departureDate\", expr(\"date_add(sasDate, depdate)\")) \\\n",
    "    .drop(\"sasDate\", \"arrdate\", \"depdate\", \"count\", \"admnum\", \"dtadfile\", \"visapost\", \"occup\", \"dtaddto\", \"insnum\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Function definition to load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def create_airport_table(spark, input_df , output_path):\n",
    "    '''\n",
    "    '''\n",
    "    # airport = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\").load(filename)\n",
    "    airport = input_df   \n",
    "\n",
    "    # filtering only US airports, data splitting country and state by \"-\" from the iso_region column & dropping old iso_region column  \n",
    "    airport =   airport.where(\n",
    "            (col(\"iso_country\") == \"US\") & (col(\"iata_code\").isNotNull()) & (col(\"type\").isin(\"large_airport\", \"medium_airport\", \"small_airport\"))) \\\n",
    "            .withColumn(\"state\", split(col(\"iso_region\"), \"-\")[1]) \\\n",
    "            .drop(\"local_code\", \"elevation_ft\", \"iso_region\", 'continent') \\\n",
    "            .dropDuplicates()\n",
    "    \n",
    "    airport = airport.select(['ident', 'type', 'name', 'state', 'municipality','gps_code', 'iata_code','iso_country', 'coordinates']) \\\n",
    "               .dropDuplicates().dropna()\n",
    "    \n",
    "    airport.write.mode(\"overwrite\").parquet(output_path+\"airport.parquet\")\n",
    "    \n",
    "  \n",
    "    \n",
    "    return airport\n",
    "\n",
    "\n",
    "def create_visa_type(df, output_path):\n",
    "    '''\n",
    "    '''\n",
    "    # create visatype df from visatype column\n",
    "    visatype_df = df.select(['visatype']).distinct()\n",
    "\n",
    "    # add an id column\n",
    "    visatype_df = visatype_df.withColumn('visa_type_key', monotonically_increasing_id())\n",
    "\n",
    "    visatype_df.write.mode(\"overwrite\").parquet(output_path+\"visa_type.parquet\")\n",
    "\n",
    "    return visatype_df\n",
    "\n",
    "\n",
    "def create_state_demographics(spark,input_df, output_path):\n",
    "    \"\"\"This function creates a us demographics dimension table from the us cities demographics data.\n",
    "    :param df: spark dataframe of us demographics survey data\n",
    "    :param output_data: path to write dimension dataframe to\n",
    "    :return: spark dataframe representing demographics dimension\n",
    "    \"\"\"\n",
    "    #cities = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(input_path)\n",
    "    cities = input_df\n",
    "    \n",
    "    state_demographics = cities \\\n",
    "        .groupBy(col(\"State Code\").alias(\"stateCode\"), col(\"State\").alias(\"state\")).agg(\n",
    "        round(mean('Median Age'), 2).alias(\"medianAge\"),\\\n",
    "        sum(\"Total Population\").alias(\"totalPopulation\"),\\\n",
    "        sum(\"Male Population\").alias(\"malePopulation\"), \\\n",
    "        sum(\"Female Population\").alias(\"femalePopulation\"),\\\n",
    "        sum(\"Number of Veterans\").alias(\"numberOfVeterans\"),\\\n",
    "        sum(\"Foreign-born\").alias(\"foreignBorn\"), \\\n",
    "        round(mean(\"Average Household Size\"),2).alias(\"averageHouseholdSize\")\n",
    "        ).dropna()\n",
    "    # lets add an id column\n",
    "    state_demographics = state_demographics.withColumn('id', monotonically_increasing_id())\n",
    "\n",
    "    # write dimension to parquet file\n",
    "    state_demographics.write.parquet(output_path + \"demographics.parquet\", mode=\"overwrite\")\n",
    "\n",
    "    return state_demographics\n",
    "\n",
    "def create_time(df, output_path):\n",
    "    time_table = df.select(['arrivalDate'])\\\n",
    "                    .withColumnRenamed('arrivalDate','time') \n",
    "\n",
    "    time_table = time_table \\\n",
    "                 .withColumn('arrival_day', func.dayofmonth('time')) \\\n",
    "                 .withColumn('arrival_month', func.month('time')) \\\n",
    "                 .withColumn('arrival_year', func.year('time')) \\\n",
    "                 .withColumn('arrival_week', func.weekofyear('time')) \\\n",
    "                 .withColumn('arrival_weekday', func.dayofweek('time'))\\\n",
    "    \n",
    "    time_table = time_table.dropDuplicates()\n",
    "\n",
    "    # write the calendar dimension to parquet file\n",
    "    partition_columns = ['arrival_year', 'arrival_month', 'arrival_week']\n",
    "    time_table.write.parquet(output_path + \"immigration_calendar.parquet\", partitionBy=partition_columns, mode=\"overwrite\")\n",
    "\n",
    "    return time_table\n",
    "\n",
    "def create_immigration_facts(df, visatype_df, state_demographics_df, time_df, airport_df, outpath):\n",
    "\n",
    "    immigration = df.select(['cicid', 'arrivalDate','mode','bornCountry', 'airline','flightNumber','visa','visaType',\n",
    "                         'gender','arrivalPort', 'matchFlag', 'year', 'month', 'birthyear', 'residentCountry', 'state_code']) \\\n",
    "                .dropna() \\\n",
    "                .dropDuplicates(['cicid'])\n",
    "\n",
    "    immigration = immigration.join(visatype_df, (visatype_df.visatype == immigration.visaType) , how = 'inner') \\\n",
    "                      .drop('visatype') \\\n",
    "                      .drop('visa') \\\n",
    "                      .dropna() \\\n",
    "                      .dropDuplicates()\n",
    "\n",
    "    immigration = immigration.join(state_demographics_df.select(['stateCode','id']), (state_demographics_df.stateCode == immigration.state_code) , how = 'left') \\\n",
    "                      .drop('state_code').drop('stateCode').drop('state').withColumnRenamed(\"id\", \"state_code_id\").dropna().dropDuplicates()\n",
    "    '''\n",
    "    immigration = immigration.join(select(['time']), (time_df.time == immigration.arrivalDate), how = 'inner') \\\n",
    "                      .drop('arrdate') \\\n",
    "                      .dropna() \\\n",
    "                      .dropDuplicates()\n",
    "    '''\n",
    "\n",
    "    immigration = immigration.join(airport_df.select(['iata_code', 'ident']), (airport_df.iata_code == immigration.arrivalPort), how = 'left') \\\n",
    "                      .drop('iata_code').withColumnRenamed(\"ident\", \"airport_id\").dropna().dropDuplicates()\n",
    "    \n",
    "    \n",
    "    # write dimension to parquet file\n",
    "    partition_columns = ['year', 'month']\n",
    "    immigration.write.parquet(outpath + \"immigration\", partitionBy=partition_columns, mode=\"overwrite\")\n",
    "    \n",
    "    return immigration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def create_spark_session():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def visualize_missing_values(df):\n",
    "    \"\"\"Visualize missing values in a spark dataframe\n",
    "    \n",
    "    :param df: spark dataframe\n",
    "    \"\"\"\n",
    "    # create a dataframe with missing values count per column\n",
    "    df = df.toPandas()\n",
    "    \n",
    "    nulls_df = pd.DataFrame(data= df.isnull().sum(), columns=['values'])\n",
    "    nulls_df = nulls_df.reset_index()\n",
    "    nulls_df.columns = ['cols', 'values']\n",
    "\n",
    "    # calculate % missing values\n",
    "    nulls_df['% missing values'] = 100*nulls_df['values']/df.shape[0]\n",
    "    \n",
    "    #Convert to spark before sending\n",
    "    print(nulls_df)\n",
    "    \n",
    "def clean_immigration_data(df):\n",
    "    \"\"\"Clean immigration dataframe\n",
    "    :param df: spark dataframe with monthly immigration data\n",
    "    :return: clean dataframe\n",
    "    \"\"\"\n",
    "    total_records = df.count()\n",
    "    \n",
    "    print(f'Total records in dataframe: {total_records:,}')\n",
    "    \n",
    "    # EDA has shown these columns to exhibit over 90% missing values, and hence we drop them\n",
    "    drop_columns = ['occup', 'entdepu','insnum']\n",
    "    df = df.drop(*drop_columns)\n",
    "    \n",
    "    # drop rows where all elements are missing\n",
    "    df = df.dropna(how='all')\n",
    "\n",
    "    new_total_records = df.count()\n",
    "    \n",
    "    print(f'Total records after cleaning: {new_total_records:,}')\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_demographics_data(df):\n",
    "    \"\"\"Clean the US demographics dataset\n",
    "    \n",
    "    :param df: spark dataframe of US demographics dataset\n",
    "    :return: clean dataframe\n",
    "    \"\"\"\n",
    "    # drop rows with missing values\n",
    "    subset_cols = [\n",
    "        'Male Population',\n",
    "        'Female Population',\n",
    "        'Number of Veterans',\n",
    "        'Foreign-born',\n",
    "        'Average Household Size'\n",
    "    ]\n",
    "    new_df = df.dropna(subset=subset_cols)\n",
    "    \n",
    "    rows_dropped = df.count()-new_df.count()\n",
    "    print(\"Rows dropped with missing values: {}\".format(rows_dropped))\n",
    "    \n",
    "    # drop duplicate columns\n",
    "    new_df2 = new_df.dropDuplicates(subset=['City', 'State', 'State Code', 'Race'])\n",
    "    \n",
    "    rows_dropped_with_duplicates = new_df.count()-new_df2.count()\n",
    "    print(f\"Rows dropped after accounting for duplicates: {rows_dropped_with_duplicates}\")\n",
    "    \n",
    "    return new_df2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### 3.2 Mapping Out Data Pipelines\n",
    "The pipeline steps are as follows:\n",
    "\n",
    "- Load the datasets\n",
    "- Clean the I94 Immigration data\n",
    "- Create visa_type dimension table\n",
    "- Create time dimension table\n",
    "- Extract clean airport data\n",
    "- Create airport table\n",
    "- Load demographics data\n",
    "- Clean demographics data\n",
    "- Create demographic dimension table\n",
    "- Create immigration fact table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        cols  values  % missing values\n",
      "0      cicid       0          0.000000\n",
      "1      i94yr       0          0.000000\n",
      "2     i94mon       0          0.000000\n",
      "3     i94cit       0          0.000000\n",
      "4     i94res       0          0.000000\n",
      "5    i94port       0          0.000000\n",
      "6    arrdate       0          0.000000\n",
      "7    i94mode       0          0.000000\n",
      "8    i94addr      13          4.012346\n",
      "9    depdate      13          4.012346\n",
      "10    i94bir       0          0.000000\n",
      "11   i94visa       0          0.000000\n",
      "12     count       0          0.000000\n",
      "13  dtadfile       0          0.000000\n",
      "14  visapost     200         61.728395\n",
      "15     occup     324        100.000000\n",
      "16   entdepa       0          0.000000\n",
      "17   entdepd      13          4.012346\n",
      "18   entdepu     324        100.000000\n",
      "19   matflag      13          4.012346\n",
      "20   biryear       0          0.000000\n",
      "21   dtaddto       0          0.000000\n",
      "22    gender      44         13.580247\n",
      "23    insnum     319         98.456790\n",
      "24   airline       8          2.469136\n",
      "25    admnum       0          0.000000\n",
      "26     fltno       3          0.925926\n",
      "27  visatype       0          0.000000\n",
      "Total records in dataframe: 324\n",
      "Total records after cleaning: 324\n"
     ]
    }
   ],
   "source": [
    "spark = create_spark_session()\n",
    "\n",
    "# Load Immigration data\n",
    "df = spark.read.parquet(\"sas_data\")\n",
    "\n",
    "# Create a smaller sample for quick run\n",
    "df = df.sample(True, 0.0001)\n",
    "\n",
    "# Explore Immigration data\n",
    "visualize_missing_values(df)\n",
    "\n",
    "# Clean data\n",
    "df = clean_immigration_data(df)\n",
    "\n",
    "# rename and load.\n",
    "df = load_immigration_data(df, spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            cols  values  % missing values\n",
      "0          ident       0          0.000000\n",
      "1           type       0          0.000000\n",
      "2           name       0          0.000000\n",
      "3   elevation_ft     239          1.050226\n",
      "4      continent       0          0.000000\n",
      "5    iso_country       0          0.000000\n",
      "6     iso_region       0          0.000000\n",
      "7   municipality     102          0.448214\n",
      "8       gps_code    1773          7.791009\n",
      "9      iata_code   20738         91.128005\n",
      "10    local_code    1521          6.683658\n",
      "11   coordinates       0          0.000000\n"
     ]
    }
   ],
   "source": [
    "airport_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \",\").load(\"airport-codes_csv.csv\")\n",
    "\n",
    "visualize_missing_values(airport_df.where(\n",
    "            (col(\"iso_country\") == \"US\")))\n",
    "\n",
    "# Between iata_code and local_code, the airports can be uniquely identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      cols  values  % missing values\n",
      "0                     City       0          0.000000\n",
      "1                    State       0          0.000000\n",
      "2               Median Age       0          0.000000\n",
      "3          Male Population       3          0.103770\n",
      "4        Female Population       3          0.103770\n",
      "5         Total Population       0          0.000000\n",
      "6       Number of Veterans      13          0.449671\n",
      "7             Foreign-born      13          0.449671\n",
      "8   Average Household Size      16          0.553442\n",
      "9               State Code       0          0.000000\n",
      "10                    Race       0          0.000000\n",
      "11                   Count       0          0.000000\n",
      "Rows dropped with missing values: 16\n",
      "Rows dropped after accounting for duplicates: 0\n"
     ]
    }
   ],
   "source": [
    "demographics_df = spark.read.format(\"csv\").option(\"header\", \"true\").option(\"delimiter\", \";\").load(\"us-cities-demographics.csv\")\n",
    "\n",
    "visualize_missing_values(demographics_df)\n",
    "\n",
    "demographics_df = clean_demographics_data(demographics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Create data models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create the dimension tables and write them as parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "output_path = \"./sas_data_out/\"\n",
    "\n",
    "airports_df = create_airport_table(spark, airport_df, output_path)\n",
    "\n",
    "visa_type_df = create_visa_type(df, output_path)\n",
    "\n",
    "state_demographics_df = create_state_demographics(spark, demographics_df, output_path)\n",
    "\n",
    "time_df = create_time(df, output_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Create the fact tables and write them as parquet file, partitioned by year and month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "immigration_facts_df = create_immigration_facts(df=df, airport_df=airports_df, \n",
    "                                                visatype_df=visa_type_df, \n",
    "                                                state_demographics_df=state_demographics_df, time_df=time_df, outpath=output_path) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Check Data Models - Data Quality check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality check for dimension tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "def data_len_check(df):\n",
    "    '''\n",
    "    This function checks if the number of the columns in the dataframe is 0 or not. \n",
    "    This function also chceks if the number of records in the dataframe is 0 or not.\n",
    "    '''\n",
    "    df_len = df.count()\n",
    "    df_col_len = len(df.columns)\n",
    "    return df_col_len <= 0 or df_len <= 0\n",
    "\n",
    "def count_missings(spark_df,sort=True):\n",
    "    \"\"\"\n",
    "    Counts number of nulls and nans in each column\n",
    "    \"\"\"\n",
    "    df = spark_df.select([func.count(func.when(func.isnan(c) | func.isnull(c), c)).alias(c) for (c,c_type) in spark_df.dtypes \n",
    "                          if c_type not in ('timestamp', 'string', 'date')]).toPandas()\n",
    "\n",
    "    if len(df) == 0:\n",
    "        print(\"There are no any missing values!\")\n",
    "        return None\n",
    "\n",
    "    if sort:\n",
    "        return df.rename(index={0: 'count'}).T.sort_values(\"count\",ascending=False)\n",
    "\n",
    "    return df\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print a warning if any of the dimension table(s) has no data\n",
    "\n",
    "if data_len_check(airports_df):\n",
    "    print(\"Airports Table data check failed\")\n",
    "elif data_len_check(visa_type_df):\n",
    "    print(\"Visa Type Table data check failed\")\n",
    "elif data_len_check(state_demographics_df):\n",
    "    print(\"State Demographic Table data check failed\")\n",
    "elif data_len_check(time_df):\n",
    "    print(\"Time Table data check failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Data Quality check for fact tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Print a warning if any of the fact table(s) has no data\n",
    "\n",
    "if data_len_check(immigration_facts_df):\n",
    "    print(\"Immigration facts data check failed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 count\n",
      "cicid                0\n",
      "mode                 0\n",
      "bornCountry          0\n",
      "year                 0\n",
      "month                0\n",
      "birthyear            0\n",
      "residentCountry      0\n",
      "visa_type_key        0\n",
      "state_code_id        0\n"
     ]
    }
   ],
   "source": [
    "# print a summary of missing data on the fact table so the user can see if the \n",
    "\n",
    "print(count_missings(immigration_facts_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "##### Thank you #######"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
